#!/usr/bin/env python3
"""
Aggregate multiple sources per area and publish RSS, Atom, JSON.
Outputs: feeds/<area>.xml, feeds/<area>.atom.xml, feeds/<area>.json
Config:  feeds.yaml
"""

import html, json, email.utils, yaml, requests, os
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timezone
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
CONFIG = ROOT / "feeds.yaml"
OUTDIR = ROOT / "feeds"
OUTDIR.mkdir(exist_ok=True)

# Browser-like headers to reduce 403/blocks
HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/125.0.0.0 Safari/537.36 4thWaveAI-Feeds/1.3"),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.7",
    "Connection": "keep-alive",
    "Cache-Control": "no-cache",
}

def fetch(url):
    r = requests.get(url, timeout=30, headers=HEADERS)
    r.raise_for_status()
    return r.text

def pick_links(index_html, base, preferred_prefix, limit=20):
    """Pick article links on the same host; prefer prefix; fall back to common patterns."""
    soup = BeautifulSoup(index_html, "lxml")
    host = urlparse(base).netloc
    seen, out = set(), []

    def add_if_good(href):
        if not href:
            return
        full = urljoin(base, href.strip())
        if urlparse(full).netloc != host:
            return
        if full in seen:
            return
        seen.add(full)
        out.append(full)

    # 1) Preferred prefix (from config)
    if preferred_prefix:
        for a in soup.select(f'a[href^="{preferred_prefix}"]'):
            add_if_good(a.get("href"))
            if len(out) >= limit: return out

    # 2) Domain-specific fallbacks
    patterns = []
    if "nanowerk.com" in host:
        patterns += ["/news2/"]
    if "phys.org" in host:
        patterns += ["/news/"]
    if "sciencedaily.com" in host:
        patterns += ["/releases/"]
    if "news.mit.edu" in host or "berkeley.edu" in host:
        patterns += ["/20"]  # year-prefixed paths like /2025/...

    for p in patterns:
        for a in soup.select(f'a[href^="{p}"]'):
            add_if_good(a.get("href"))
            if len(out) >= limit: return out

    # 3) Generic fallback: any same-host link that looks like an article
    for a in soup.select("a[href]"):
        href = a.get("href", "")
        if any(k in href for k in ("/news/", "/story/", "/releases/", "/202", "/20")):
            add_if_good(href)
            if len(out) >= limit: break

    return out[:limit]

def parse_article(url):
    try:
        s = BeautifulSoup(fetch(url), "lxml")
        ogt = s.find("meta", property="og:title")
        title = ogt.get("content", "").strip() if ogt else (s.title.get_text(strip=True) if s.title else url)

        ogd = s.find("meta", property="og:description")
        if ogd and ogd.get("content"):
            description = ogd["content"].strip()
        else:
            p = (s.find("article") or s).find("p")
            description = (p.get_text(" ", strip=True) if p else "")[:400]

        pub = s.find("meta", property="article:published_time")
        pubDate = None
        if pub and pub.get("content"):
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(pub["content"].replace("Z", "+00:00"))
                pubDate = email.utils.format_datetime(dt)
            except Exception:
                pubDate = None

        return {"title": title, "link": url, "guid": url, "description": description, "pubDate": pubDate}
    except Exception as e:
        print(f"Skip {url}: {e}")
        return None

def build_rss(items, title, home_url):
    now_rfc = email.utils.format_datetime(datetime.now(timezone.utc))
    parts = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<rss version="2.0">',
        "  <channel>",
        f"    <title>{html.escape(title)}</title>",
        f"    <link>{html.escape(home_url)}</link>",
        "    <description>Aggregated feed generated by 4thWave AI.</description>",
        "    <language>en-us</language>",
        f"    <lastBuildDate>{now_rfc}</lastBuildDate>",
    ]
    for it in items:
        parts += [
            "    <item>",
            f"      <title>{html.escape(it['title'])}</title>",
            f"      <link>{html.escape(it['link'])}</link>",
            f"      <guid isPermaLink=\"true\">{html.escape(it['guid'])}</guid>",
            f"      <description><![CDATA[{it['description']}]]></description>",
        ]
        if it.get("pubDate"):
            parts.append(f"      <pubDate>{it['pubDate']}</pubDate>")
        parts += ["    </item>"]
    parts += ["  </channel>", "</rss>"]
    return "\n".join(parts)

def build_atom(items, title, self_url, home_url, feed_id):
    now_rfc = email.utils.format_datetime(datetime.now(timezone.utc))
    parts = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<feed xmlns="http://www.w3.org/2005/Atom">',
        f"  <title>{html.escape(title)}</title>",
        f"  <link href=\"{html.escape(home_url)}\"/>",
        f"  <link rel=\"self\" href=\"{html.escape(self_url)}\"/>",
        f"  <id>{html.escape(feed_id)}</id>",
        f"  <updated>{now_rfc}</updated>",
    ]
    for it in items:
        parts += [
            "  <entry>",
            f"    <title>{html.escape(it['title'])}</title>",
            f"    <link href=\"{html.escape(it['link'])}\"/>",
            f"    <id>{html.escape(it['guid'])}</id>",
            f"    <summary type=\"html\"><![CDATA[{it['description']}]]></summary>",
        ]
        if it.get("pubDate"):
            parts.append(f"    <updated>{it['pubDate']}</updated>")
        parts += ["  </entry>"]
    parts += ["</feed>"]
    return "\n".join(parts)

def build_json(items, title, self_url, home_url):
    feed = {
        "version": "https://jsonfeed.org/version/1",
        "title": title,
        "home_page_url": home_url,
        "feed_url": self_url,
        "items": []
    }
    for it in items:
        feed["items"].append({
            "id": it["guid"],
            "url": it["link"],
            "title": it["title"],
            "content_text": it["description"]
        })
    return json.dumps(feed, ensure_ascii=False, indent=2)

def main():
    cfg = yaml.safe_load(CONFIG.read_text(encoding="utf-8"))
    site_base = cfg.get("site_base", "https://4thwaveai-feeds.github.io/4thwaveai-feeds/")
    home_url  = cfg.get("home_url", "https://4thwave.ai")
    max_items = cfg.get("max_items_per_area", 60)

    for area, sources in cfg["areas"].items():
        collected = []
        for src in sources:
            try:
                idx = fetch(src["index"])
                links = pick_links(idx, src["base"], src.get("prefix", "/"), limit=src.get("limit", 10))
                for u in links:
                    item = parse_article(u)
                    if item:
                        collected.append(item)
            except Exception as e:
                print(f"Source fail ({area}): {src.get('name', src['base'])} -> {e}")
                continue

        # de-dup
        seen, unique = set(), []
        for it in collected:
            if it["guid"] in seen: 
                continue
            seen.add(it["guid"])
            unique.append(it)

        unique = unique[:max_items]
        title = f"4thWave AI â€” {area.title()} (Aggregated)"

        # If nothing parsed, DO NOT overwrite existing files (prevents empty feeds)
        if not unique:
            print(f"No items for area '{area}'; skipping write to preserve previous feed.")
            continue

        rss_path  = OUTDIR / f"{area}.xml"
        atom_path = OUTDIR / f"{area}.atom.xml"
        json_path = OUTDIR / f"{area}.json"

        rss_xml  = build_rss(unique, title, home_url)
        atom_xml = build_atom(unique, title, site_base + f"feeds/{area}.atom.xml", home_url, f"urn:4thwaveai-feeds:{area}")
        json_txt = build_json(unique, title, site_base + f"feeds/{area}.json", home_url)

        rss_path.write_text(rss_xml, encoding="utf-8")
        atom_path.write_text(atom_xml, encoding="utf-8")
        json_path.write_text(json_txt, encoding="utf-8")

        print(f"Wrote {area}: {len(unique)} items")

if __name__ == "__main__":
    main()
