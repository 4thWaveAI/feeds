#!/usr/bin/env python3
"""
Aggregate multiple sources per area and publish RSS, Atom, JSON.
Outputs go to repo path: feeds/<area>.{xml,atom.xml,json}
Config file: feeds.yaml  (editable list of sources per area)
"""

import html, json, email.utils, yaml, requests, os
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timezone
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
CONFIG = ROOT / "feeds.yaml"
OUTDIR = ROOT / "feeds"
OUTDIR.mkdir(exist_ok=True)

USER_AGENT = "4thWaveAI-Aggregator/1.0 (+https://github.com/4thwaveai-feeds/4thwaveai-feeds)"

def fetch(url):
    r = requests.get(url, timeout=30, headers={"User-Agent": USER_AGENT})
    r.raise_for_status()
    return r.text

def pick_links(index_html, base, path_prefix, limit=20):
    soup = BeautifulSoup(index_html, "lxml")
    out, seen = [], set()
    for a in soup.select("a[href]"):
        href = a.get("href", "").strip()
        if not href:
            continue
        full = urljoin(base, href)
        # keep links on same host and under the given prefix
        if urlparse(full).netloc != urlparse(base).netloc:
            continue
        if path_prefix and path_prefix not in full:
            continue
        if full not in seen:
            seen.add(full)
            out.append(full)
        if len(out) >= limit:
            break
    return out

def parse_article(url):
    try:
        s = BeautifulSoup(fetch(url), "lxml")
        ogt = s.find("meta", property="og:title")
        title = ogt.get("content", "").strip() if ogt else (s.title.get_text(strip=True) if s.title else url)

        ogd = s.find("meta", property="og:description")
        if ogd and ogd.get("content"):
            description = ogd["content"].strip()
        else:
            p = (s.find("article") or s).find("p")
            description = (p.get_text(" ", strip=True) if p else "")[:400]

        pub = s.find("meta", property="article:published_time")
        pubDate = None
        if pub and pub.get("content"):
            try:
                dt = datetime.fromisoformat(pub["content"].replace("Z", "+00:00"))
                pubDate = email.utils.format_datetime(dt)
            except Exception:
                pass

        return {"title": title, "link": url, "guid": url, "description": description, "pubDate": pubDate}
    except Exception:
        return None

def build_rss(items, title, home_url):
    now_rfc = email.utils.format_datetime(datetime.now(timezone.utc))
    parts = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<rss version="2.0">',
        "  <channel>",
        f"    <title>{html.escape(title)}</title>",
        f"    <link>{html.escape(home_url)}</link>",
        "    <description>Aggregated feed generated by 4thWave AI.</description>",
        "    <language>en-us</language>",
        f"    <lastBuildDate>{now_rfc}</lastBuildDate>",
    ]
    for it in items:
        parts += [
            "    <item>",
            f"      <title>{html.escape(it['title'])}</title>",
            f"      <link>{html.escape(it['link'])}</link>",
            f"      <guid isPermaLink=\"true\">{html.escape(it['guid'])}</guid>",
            f"      <description><![CDATA[{it['description']}]]></description>",
        ]
        if it.get("pubDate"):
            parts.append(f"      <pubDate>{it['pubDate']}</pubDate>")
        parts += ["    </item>"]
    parts += ["  </channel>", "</rss>"]
    return "\n".join(parts)

def build_atom(items, title, self_url, home_url, feed_id):
    now_rfc = email.utils.format_datetime(datetime.now(timezone.utc))
    parts = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<feed xmlns="http://www.w3.org/2005/Atom">',
        f"  <title>{html.escape(title)}</title>",
        f"  <link href=\"{html.escape(home_url)}\"/>",
        f"  <link rel=\"self\" href=\"{html.escape(self_url)}\"/>",
        f"  <id>{html.escape(feed_id)}</id>",
        f"  <updated>{now_rfc}</updated>",
    ]
    for it in items:
        parts += [
            "  <entry>",
            f"    <title>{html.escape(it['title'])}</title>",
            f"    <link href=\"{html.escape(it['link'])}\"/>",
            f"    <id>{html.escape(it['guid'])}</id>",
            f"    <summary type=\"html\"><![CDATA[{it['description']}]]></summary>",
        ]
        if it.get("pubDate"):
            parts.append(f"    <updated>{it['pubDate']}</updated>")
        parts += ["  </entry>"]
    parts += ["</feed>"]
    return "\n".join(parts)

def build_json(items, title, self_url, home_url):
    feed = {
        "version": "https://jsonfeed.org/version/1",
        "title": title,
        "home_page_url": home_url,
        "feed_url": self_url,
        "items": []
    }
    for it in items:
        feed["items"].append({
            "id": it["guid"],
            "url": it["link"],
            "title": it["title"],
            "content_text": it["description"]
        })
    return json.dumps(feed, ensure_ascii=False, indent=2)

def main():
    cfg = yaml.safe_load(CONFIG.read_text(encoding="utf-8"))
    site_base = cfg.get("site_base", "https://4thwaveai-feeds.github.io/4thwaveai-feeds/")
    for area, sources in cfg["areas"].items():
        collected = []
        for src in sources:
            try:
                idx = fetch(src["index"])
                links = pick_links(idx, src["base"], src.get("prefix", "/"), limit=src.get("limit", 10))
                for u in links:
                    item = parse_article(u)
                    if item:
                        collected.append(item)
            except Exception:
                continue
        # de-dup by GUID and cap total
        seen, unique = set(), []
        for it in collected:
            if it["guid"] in seen: 
                continue
            seen.add(it["guid"])
            unique.append(it)
        unique = unique[: cfg.get("max_items_per_area", 60) ]

        # paths + titles
        title = f"4thWave AI â€” {area.title()} (Aggregated)"
        rss_path  = OUTDIR / f"{area}.xml"
        atom_path = OUTDIR / f"{area}.atom.xml"
        json_path = OUTDIR / f"{area}.json"

        rss_xml  = build_rss(unique, title, cfg.get("home_url", "https://4thwave.ai"))
        atom_xml = build_atom(unique, title, site_base + f"feeds/{area}.atom.xml",
                              cfg.get("home_url", "https://4thwave.ai"),
                              f"urn:4thwaveai-feeds:{area}")
        json_txt = build_json(unique, title, site_base + f"feeds/{area}.json",
                              cfg.get("home_url", "https://4thwave.ai"))

        rss_path.write_text(rss_xml, encoding="utf-8")
        atom_path.write_text(atom_xml, encoding="utf-8")
        json_path.write_text(json_txt, encoding="utf-8")

        print(f"Wrote {area}: {len(unique)} items")

if __name__ == "__main__":
    main()
